# File: AP-09.md

**REF-ID:** AP-09  
**Version:** 1.0.0  
**Category:** Anti-Patterns  
**Type:** Dependencies  
**Severity:** üî¥ Critical  
**Status:** Active

---

## SUMMARY

Adding heavy dependencies (pandas, numpy, scipy) without justification. Lambda has 128MB deployment size limit; heavy libraries quickly exceed this.

---

## THE ANTI-PATTERN

```python
# ‚ùå HEAVY DEPENDENCIES WITHOUT JUSTIFICATION
import pandas as pd  # ~100MB
import numpy as np   # ~50MB
import scipy         # ~80MB

def process_data(data):
    df = pd.DataFrame(data)
    return df.mean()
```

---

## WHY IT'S WRONG

**Lambda Constraints:**
- 128MB deployment package limit (DEC-07)
- Cold start time increases with size
- Memory usage increases

**Library Sizes:**
```
pandas: ~100MB
numpy: ~50MB
scipy: ~80MB
TensorFlow: ~500MB
```

---

## WHAT TO DO INSTEAD

```python
# ‚úÖ USE STANDARD LIBRARY
import statistics

def process_data(data):
    return statistics.mean(data)
```

---

## DECISION FRAMEWORK

**Before Adding Library:**

1. **Is it necessary?** Can standard library work?
2. **What's the size?** Check package size
3. **What's the benefit?** Worth the cost?
4. **Are there alternatives?** Lighter options?

---

## ALTERNATIVES

| Heavy Library | Lighter Alternative |
|---------------|---------------------|
| pandas | json, csv modules |
| numpy | math, statistics |
| requests | urllib3, http.client |
| PIL/Pillow | Consider external processing |

---

## IF JUSTIFIED

**Document the decision:**
```python
# DEC-##: Using pandas for complex data transformation
# Justified because:
# - Processing 10K+ row datasets
# - Complex joins required
# - No lighter alternative
# - Measured cold start: +2s acceptable
import pandas as pd
```

---

## DETECTION

```bash
# Check deployment size
du -h deployment-package.zip

# Should be well under 128MB
```

---

## RELATED

- **DEC-07** - 128MB Limit
- **AP-12** - Premature Optimization

---

**Keywords:** dependencies, Lambda limits, deployment size, heavy libraries

---

**END OF AP-09**
