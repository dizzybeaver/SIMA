# AP-13.md - Multiprocessing in Limited Environments

**REF-ID:** AP-13  
**Version:** 1.0.0  
**Category:** Anti-Patterns  
**Topic:** Concurrency  
**Priority:** üü† HIGH  
**Status:** Active  
**Created:** 2025-10-30  
**Last Updated:** 2025-10-30

---

## Summary

Using multiprocessing in constrained execution environments (serverless, containers with limited resources). Process creation overhead and resource limits make this approach ineffective.

---

## Context

Developers try to parallelize work within a single invocation using multiprocessing. In resource-constrained environments, this adds complexity without benefit and may violate platform constraints.

**Problem:** High overhead, no real parallelism, platform violations, wasted resources.

---

## Content

### The Anti-Pattern

```python
# ‚ùå MULTIPROCESSING IN SERVERLESS ENVIRONMENT
from multiprocessing import Pool

def process_batch(items):
    with Pool(4) as pool:  # Won't work as expected!
        results = pool.map(process_item, items)
    return results

# Problems:
# 1. Process creation overhead (50-100ms per process)
# 2. Limited CPU allocation (often < 1 vCPU)
# 3. Memory constraints
# 4. No real parallelism benefit
```

### Why This Is Wrong

**Serverless/Container Constraints:**
```
Resource allocation:
- CPU: Often 0.5-1 vCPU (fractional!)
- Memory: Limited (128MB-3GB typical)
- Execution time: Time-limited
- Cold start penalty: Process creation adds latency

With Pool(4):
- Creates 4 processes
- Each needs memory allocation
- CPU time-sliced across 4 processes
- No speed benefit (same total CPU)
- Added overhead: 200-400ms
```

**Process Creation Overhead:**
```python
import time
from multiprocessing import Pool

# Overhead measurement
start = time.time()
with Pool(4) as pool:
    pass  # Do nothing
overhead = time.time() - start
# Overhead: 100-200ms just for setup

# For small workloads:
# - Sequential: 50ms work + 0ms overhead = 50ms
# - Multiprocessing: 50ms work + 150ms overhead = 200ms
# Result: 4x slower!
```

### Correct Approaches

**Approach 1: Sequential Processing**
```python
# ‚úÖ SEQUENTIAL - Often fast enough
def process_batch(items):
    results = []
    for item in items:
        results.append(process_item(item))
    return results

# When appropriate:
# - Small batches (< 100 items)
# - Fast operations (< 10ms each)
# - Constrained environment
```

**Approach 2: Distributed Processing (Proper Parallelism)**
```python
# ‚úÖ INVOKE MULTIPLE INSTANCES
# Instead of multiprocessing within one instance,
# launch multiple independent instances

# Pseudocode for parallel invocations:
def process_batch_distributed(items):
    # Split items across multiple invocations
    batch_size = 10
    batches = [items[i:i+batch_size] 
               for i in range(0, len(items), batch_size)]
    
    # Each batch processed by separate invocation
    futures = []
    for batch in batches:
        future = invoke_async(process_batch, batch)
        futures.append(future)
    
    # Collect results
    return [f.result() for f in futures]
```

**Approach 3: Queue-Based Processing**
```python
# ‚úÖ MESSAGE QUEUE + MULTIPLE WORKERS
# Producer: Split work into messages
def split_work(items):
    for item in items:
        queue.send_message(item)

# Multiple worker instances consume messages in parallel
def worker():
    while True:
        message = queue.receive_message()
        if message:
            process_item(message)
```

**Approach 4: Step Functions / Workflow**
```python
# ‚úÖ ORCHESTRATION SERVICE
# Define parallel execution in workflow
{
  "States": {
    "ProcessItems": {
      "Type": "Map",
      "ItemsPath": "$.items",
      "Iterator": {
        "StartAt": "ProcessSingle",
        "States": {
          "ProcessSingle": {
            "Type": "Task",
            "Resource": "arn:aws:lambda:...:function:ProcessItem"
          }
        }
      }
    }
  }
}
```

### Decision Matrix

| Scenario | Best Approach | Why |
|----------|---------------|-----|
| < 50 items, < 1s total | Sequential | Overhead not worth it |
| 50-500 items, I/O bound | Async/await (if supported) | Better than processes |
| > 500 items | Distributed (multiple invocations) | True parallelism |
| Streaming | Queue-based | Continuous processing |
| Complex workflow | Orchestration service | Managed parallelism |

### Detection

```bash
# Find multiprocessing usage
grep -r "from multiprocessing import" src/
grep -r "import multiprocessing" src/
grep -r "Pool(" src/
grep -r "Process(" src/

# Should return ZERO in constrained environments
```

### Cost Considerations

**Multiprocessing within invocation:**
```
Single invocation: $0.0000002 √ó 500ms = $0.0000001
But: Wasted on overhead, no speed gain
```

**Multiple invocations (distributed):**
```
10 invocations: $0.0000002 √ó 100ms √ó 10 = $0.0000002
Benefit: 5x faster (true parallelism)
Cost: 2x higher (acceptable for 5x speedup)
```

---

## Related Topics

- **AP-08**: Threading Locks - Related synchronization anti-pattern
- **AP-11**: Race Conditions - Understanding isolation
- **AP-12**: Premature Optimization - Measure before adding complexity

---

## Keywords

multiprocessing, parallelism, serverless, distributed processing, step functions, queue-based, worker pool, resource constraints

---

## Version History

- **2025-10-30**: Created - Genericized from Lambda to any constrained environment

---

**File:** `AP-13.md`  
**Lines:** ~195  
**End of Document**
