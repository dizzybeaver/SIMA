# DT-04.md

**REF-ID:** DT-04  
**Category:** Decision Logic  
**Subcategory:** Feature Addition  
**Name:** Should This Be Cached  
**Priority:** High  
**Status:** Active  
**Created:** 2024-10-30  
**Updated:** 2024-10-30

---

## Summary

Decision tree for cache vs compute trade-offs - determining when caching provides benefit based on computation cost, access frequency, data volatility, and size constraints.

---

## Problem

Caching adds complexity and memory usage. Must balance performance gains against memory constraints (128MB limit in Lambda), data staleness, and maintenance overhead.

---

## Decision Tree

```
START: Considering caching data X
│
├─ Q: Is X expensive to compute/fetch?
│  ├─ NO (<10ms) → Don't cache
│  │      Rationale: Cache overhead > computation
│  │      → END
│  │
│  └─ YES (>10ms) → Continue
│
├─ Q: Is X accessed frequently?
│  ├─ NO (once per request) → Don't cache
│  │      Rationale: No reuse benefit
│  │      → END
│  │
│  └─ YES (multiple times) → Continue
│
├─ Q: Does X change frequently?
│  ├─ YES (every request) → Don't cache
│  │      Rationale: Always stale
│  │      → END
│  │
│  └─ NO (stable) → Continue
│
├─ Q: Is X large (>1MB)?
│  ├─ YES → Cache selectively
│  │      Rationale: Memory constraint (128MB)
│  │      Decision: Cache small subset or summary
│  │      → END
│  │
│  └─ NO (<1MB) → Continue
│
└─ Decision: Cache X with appropriate TTL
   │
   ├─ Q: How often does X change?
   │  ├─ Fast (seconds) → TTL: 60s
   │  ├─ Medium (minutes) → TTL: 300s
   │  ├─ Slow (hours) → TTL: 600s
   │  └─ Never → TTL: None (manual invalidation)
   │
   └─ Implementation:
      cached = gateway.cache_get("X")
      if cached is None:
          cached = compute_X()
          gateway.cache_set("X", cached, ttl=TTL)
      return cached
      → END
```

---

## Decision Matrix

| Data Type | Expensive? | Frequent? | Stable? | Size | Decision | TTL |
|-----------|------------|-----------|---------|------|----------|-----|
| API response | YES (50ms) | YES | Medium | 10KB | Cache | 300s |
| Config value | NO (2ms) | YES | High | 1KB | Don't cache | - |
| DB query | YES (100ms) | YES | Medium | 50KB | Cache | 300s |
| User session | YES (20ms) | YES | High | 5KB | Cache | 600s |
| Request ID | NO (2ms) | NO | N/A | 50B | Don't cache | - |
| Large file | YES (500ms) | YES | High | 5MB | Cache metadata | 600s |

---

## Examples

### Example 1: Cache API Responses

**Scenario:**
External API takes 50ms to fetch, accessed 5 times per request, updates every 5 minutes

**Decision:**
- Expensive? YES (50ms > 10ms)
- Frequent? YES (5 times per request)
- Stable? YES (5 min updates)
- Size? NO (10KB < 1MB)
- **Action:** Cache with TTL=300s

**Implementation:**
```python
from gateway import cache_get, cache_set, http_get

def fetch_api_data(endpoint: str) -> dict:
    """Fetch API data with caching."""
    cache_key = f"api:{endpoint}"
    
    # Try cache first
    cached = cache_get(cache_key)
    if cached is not None:
        return cached
    
    # Fetch if not cached
    response = http_get(f"https://api.example.com/{endpoint}")
    
    # Cache with 5-minute TTL
    cache_set(cache_key, response, ttl=300)
    
    return response
```

**Benefit:**
- Without cache: 5 × 50ms = 250ms per request
- With cache: 0.5ms (cache hit) + 50ms (1 miss every 5 min)
- Savings: ~249ms per request (99.8% improvement)

### Example 2: Don't Cache - Fast Operation

**Scenario:**
UUID generation takes 2ms, used once per request

**Decision:**
- Expensive? NO (2ms < 10ms)
- **Action:** Don't cache

**Implementation:**
```python
import uuid

def generate_request_id() -> str:
    """Generate unique request ID."""
    return str(uuid.uuid4())  # 2ms, no cache needed
```

**Rationale:**
- Cache overhead: ~0.5ms (get) + 0.5ms (set) = 1ms
- Direct generation: 2ms
- Benefit: Only 1ms saved, not worth cache complexity

### Example 3: Don't Cache - Volatile Data

**Scenario:**
User input validation, different every request

**Decision:**
- Expensive? Maybe (10ms)
- Frequent? NO (once per request)
- Stable? NO (always different)
- **Action:** Don't cache

**Implementation:**
```python
def validate_user_input(data: dict) -> bool:
    """Validate user input."""
    # Always validate fresh, never cache
    return perform_validation(data)
```

### Example 4: Cache Metadata Only - Large File

**Scenario:**
5MB file takes 500ms to fetch, accessed multiple times, updates hourly

**Decision:**
- Expensive? YES (500ms)
- Frequent? YES
- Stable? YES (hourly)
- Size? YES (5MB too large)
- **Action:** Cache metadata/summary, not full file

**Implementation:**
```python
from gateway import cache_get, cache_set

def get_file_info(file_id: str) -> dict:
    """Get file metadata with caching."""
    cache_key = f"file_meta:{file_id}"
    
    # Try cache first
    cached = cache_get(cache_key)
    if cached is not None:
        return cached
    
    # Fetch full file
    full_file = fetch_large_file(file_id)  # 5MB
    
    # Extract metadata only
    metadata = {
        'id': file_id,
        'size': len(full_file),
        'checksum': calculate_checksum(full_file),
        'modified': full_file['modified'],
    }  # <1KB
    
    # Cache metadata only
    cache_set(cache_key, metadata, ttl=3600)
    
    return metadata
```

---

## TTL Selection Guide

### Fast-Changing Data (TTL: 60s)
- Real-time metrics
- Live scores/prices
- Active user presence

### Medium-Changing Data (TTL: 300s)
- API responses
- Search results
- Configuration values

### Slow-Changing Data (TTL: 600s)
- User profiles
- Static content
- Reference data

### Permanent Data (No TTL)
- System constants
- Feature flags
- Manual invalidation only

---

## Anti-Patterns

### ❌ Caching Everything

**Problem:**
```python
# Caching fast operations
cache_set("uuid", generate_uuid(), ttl=300)  # 2ms operation!
cache_set("timestamp", get_time(), ttl=60)   # 0.1ms operation!
```

**Solution:**
```python
# Only cache expensive operations
uuid = generate_uuid()  # Direct, no cache
timestamp = get_time()  # Direct, no cache
```

### ❌ Wrong TTL

**Problem:**
```python
# Live stock price with 1-hour TTL
cache_set("stock_price", fetch_price(), ttl=3600)  # Stale!
```

**Solution:**
```python
# Live stock price with 60s TTL
cache_set("stock_price", fetch_price(), ttl=60)  # Fresh
```

### ❌ Caching Large Objects

**Problem:**
```python
# Caching 5MB file in Lambda (128MB limit)
cache_set("large_file", file_data, ttl=600)  # Memory exhaustion!
```

**Solution:**
```python
# Cache metadata only
metadata = extract_metadata(file_data)  # <1KB
cache_set("file_metadata", metadata, ttl=600)  # Efficient
```

---

## Related Patterns

**From NM04 (Decisions):**
- **DEC-09:** Cache design decisions

**From NM05 (Anti-Patterns):**
- **AP-12:** Premature optimization

**From NM06 (Lessons):**
- **LESS-02:** Measure, don't guess

**From NM07 (Decision Logic):**
- **DT-03:** User wants feature
- **DT-07:** Should I optimize
- **FW-01:** Cache vs compute framework

---

## Keywords

caching, cache decision, performance, TTL, memory constraints, cache strategy, optimization

---

## Version History

- **2024-10-30:** Migrated to SIMAv4 format from NM07 v3
- **2024-10-24:** Created in SIMAv3 format

---

**File:** `DT-04.md`  
**Location:** `/sima/entries/decisions/feature-addition/`  
**End of Document**
