# File: AP-09.md

**REF-ID:** AP-09  
**Version:** 1.0.0  
**Category:** Anti-Patterns  
**Type:** Dependencies  
**Severity:** ðŸ"´ Critical  
**Status:** Active

---

## SUMMARY

Adding heavy dependencies (pandas, numpy, scipy) without justification. Lambda has 128MB deployment size limit; heavy libraries quickly exceed this.

---

## THE ANTI-PATTERN

```python
# âŒ HEAVY DEPENDENCIES WITHOUT JUSTIFICATION
import pandas as pd  # ~100MB
import numpy as np   # ~50MB
import scipy         # ~80MB

def process_data(data):
    df = pd.DataFrame(data)
    return df.mean()
```

---

## WHY IT'S WRONG

**Lambda Constraints:**
- 128MB deployment package limit (DEC-07)
- Cold start time increases with size
- Memory usage increases

**Library Sizes:**
```
pandas: ~100MB
numpy: ~50MB
scipy: ~80MB
TensorFlow: ~500MB
```

---

## WHAT TO DO INSTEAD

```python
# âœ… USE STANDARD LIBRARY
import statistics

def process_data(data):
    return statistics.mean(data)
```

---

## DECISION FRAMEWORK

**Before Adding Library:**

1. **Is it necessary?** Can standard library work?
2. **What's the size?** Check package size
3. **What's the benefit?** Worth the cost?
4. **Are there alternatives?** Lighter options?

---

## ALTERNATIVES

| Heavy Library | Lighter Alternative |
|---------------|---------------------|
| pandas | json, csv modules |
| numpy | math, statistics |
| requests | urllib3, http.client |
| PIL/Pillow | Consider external processing |

---

## IF JUSTIFIED

**Document the decision:**
```python
# DEC-##: Using pandas for complex data transformation
# Justified because:
# - Processing 10K+ row datasets
# - Complex joins required
# - No lighter alternative
# - Measured cold start: +2s acceptable
import pandas as pd
```

---

## DETECTION

```bash
# Check deployment size
du -h deployment-package.zip

# Should be well under 128MB
```

---

## RELATED

- **DEC-07** - 128MB Limit
- **AP-12** - Premature Optimization

---

**Keywords:** dependencies, Lambda limits, deployment size, heavy libraries

---

**END OF AP-09**

---

# File: Dependencies-Index.md

**Category:** Anti-Patterns  
**Topic:** Dependencies  
**Items:** 1  
**Version:** 1.0.0

---

## FILE

### AP-09: Heavy Dependencies
- **Severity:** ðŸ"´ Critical
- **Problem:** Adding large libraries without justification
- **Limit:** 128MB total deployment size
- **Solution:** Use standard library or justify with measurements

---

**Keywords:** dependencies, Lambda limits, deployment size

**END OF INDEX**

---

# File: AP-10.md

**REF-ID:** AP-10  
**Version:** 1.0.0  
**Category:** Anti-Patterns  
**Type:** Critical  
**Severity:** ðŸ"´ Critical  
**Status:** Active

---

## SUMMARY

Using mutable default arguments in Python functions. This is a classic Python gotcha that causes shared state bugs.

---

## THE ANTI-PATTERN

```python
# âŒ MUTABLE DEFAULT ARGUMENT
def add_item(item, items=[]):  # DANGER!
    items.append(item)
    return items

# Usage
list1 = add_item("a")  # ["a"]
list2 = add_item("b")  # ["a", "b"] - UNEXPECTED!
list3 = add_item("c")  # ["a", "b", "c"] - BUG!
```

---

## WHY IT'S WRONG

**Default Created Once:**
- Default list created at function definition
- Same list object reused for all calls
- Modifications persist across calls
- Subtle, hard-to-find bugs

**BUG-01 Incident:**
Sentinel object (_CacheMiss) was passed as default, causing 535ms performance penalty.

---

## WHAT TO DO INSTEAD

```python
# âœ… USE None AS DEFAULT
def add_item(item, items=None):
    if items is None:
        items = []
    items.append(item)
    return items

# Usage
list1 = add_item("a")  # ["a"]
list2 = add_item("b")  # ["b"] - CORRECT!
list3 = add_item("c")  # ["c"] - CORRECT!
```

---

## APPLIES TO

**Any Mutable Type:**
- Lists: `[]`
- Dicts: `{}`
- Sets: `set()`
- Custom objects
- Sentinel objects (BUG-01)

**Safe Immutables:**
- None (preferred)
- Numbers: `0`, `1`
- Strings: `""`
- Tuples: `()`

---

## DETECTION

```python
# Lint rule
def check_defaults(func):
    defaults = func.__defaults__
    if defaults:
        for d in defaults:
            if isinstance(d, (list, dict, set)):
                raise Warning(f"Mutable default in {func.__name__}")
```

---

## REAL-WORLD INCIDENT

**BUG-01:** _CacheMiss sentinel used as default argument
- Performance: 535ms penalty
- Cause: Mutable default
- Fix: DEC-05 (Sentinel sanitization)

---

## RELATED

- **BUG-01** - Sentinel Leak
- **DEC-05** - Sentinel Sanitization
- **AP-19** - Sentinel Crossing Boundaries

---

**Keywords:** mutable defaults, Python gotcha, shared state, default arguments

---

**END OF AP-10**

---

# File: Critical-Index.md

**Category:** Anti-Patterns  
**Topic:** Critical  
**Items:** 1  
**Version:** 1.0.0

---

## FILE

### AP-10: Mutable Default Arguments
- **Severity:** ðŸ"´ Critical
- **Problem:** Using mutable defaults (lists, dicts, objects)
- **Solution:** Use None as default, create new object in function
- **Impact:** BUG-01 (535ms penalty from sentinel leak)

---

**Common Forms:**
```python
def func(x, items=[]):     # âŒ
def func(x, config={}):    # âŒ
def func(x, obj=MyObj()):  # âŒ

def func(x, items=None):   # âœ…
```

---

**Keywords:** mutable defaults, Python gotcha, critical bug

**END OF INDEX**

---

# File: AP-12.md

**REF-ID:** AP-12  
**Version:** 1.0.0  
**Category:** Anti-Patterns  
**Type:** Performance  
**Severity:** ðŸŸ¡ Medium  
**Status:** Active

---

## SUMMARY

Optimizing code before measuring performance. Premature optimization adds complexity without proven benefit.

---

## THE ANTI-PATTERN

```python
# âŒ PREMATURE OPTIMIZATION
def process_items(items):
    # "This MUST be slow, let me optimize"
    # Adds caching without measuring
    cache = {}
    results = []
    for item in items:
        key = hash(item)
        if key in cache:
            results.append(cache[key])
        else:
            result = expensive_operation(item)
            cache[key] = result
            results.append(result)
    return results
```

---

## WHY IT'S WRONG

**Problems:**
1. **Added complexity** without proven need
2. **Harder to understand** the code
3. **Premature decisions** may optimize wrong thing
4. **Time wasted** on non-bottlenecks

**Donald Knuth:**
"Premature optimization is the root of all evil."

---

## WHAT TO DO INSTEAD

```python
# âœ… MEASURE FIRST, OPTIMIZE LATER
def process_items(items):
    # Simple, clear implementation
    results = []
    for item in items:
        results.append(expensive_operation(item))
    return results

# IF profiling shows this is slow, THEN optimize
```

---

## THE OPTIMIZATION PROCESS

**1. Measure:**
```python
import time
start = time.perf_counter()
result = process_items(data)
elapsed = time.perf_counter() - start
print(f"Time: {elapsed:.3f}s")
```

**2. Profile:**
```bash
python -m cProfile -o profile.stats script.py
python -m pstats profile.stats
```

**3. Identify Bottleneck:**
- What function takes most time?
- Is it called frequently?
- Can it be optimized?

**4. Optimize Bottleneck:**
- Make targeted change
- Measure improvement
- Keep if significant

**5. Document:**
```python
# Optimized based on profiling (2025-10-30)
# Before: 5.2s, After: 0.8s (6.5x improvement)
```

---

## WHEN TO OPTIMIZE

**YES:**
- Measured bottleneck
- Significant impact
- Clear improvement
- Worth complexity

**NO:**
- No measurements
- Micro-optimization
- Adds complexity
- Unclear benefit

---

## RELATED

- **LESS-02** - Measure Don't Guess
- **AP-03** - Gateway Overhead (measured: negligible)

---

**Keywords:** premature optimization, profiling, performance, measurement

---

**END OF AP-12**

---

# File: Performance-Index.md

**Category:** Anti-Patterns  
**Topic:** Performance  
**Items:** 1  
**Version:** 1.0.0

---

## FILE

### AP-12: Premature Optimization
- **Severity:** ðŸŸ¡ Medium
- **Problem:** Optimizing before measuring
- **Quote:** "Premature optimization is the root of all evil" - Donald Knuth
- **Solution:** Measure â†' Identify bottleneck â†' Optimize â†' Measure improvement

---

**Optimization Process:**
1. Measure current performance
2. Profile to find bottleneck
3. Optimize bottleneck only
4. Measure improvement
5. Document decision

---

**Keywords:** premature optimization, profiling, performance

**END OF INDEX**
